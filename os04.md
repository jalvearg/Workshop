<h1>OpenShift Dia 4</h1>

<p>
<strong>Meta:</strong>
<br>Conocer el flujo de trabajo de openshift
</p>
<p>
<strong>Objetivos:</strong>
<br>- Automatizar el despliegue de aplicaciones con openshift
</p>
<p>
<strong>Secciones:</strong>
<br>- Gesti√≥n aplicaciones
</p>
<p>
<strong>Laboratorios:</strong>
<br><strong>-Explorando Templates</strong>
<br><strong>-Almacenamiento persistente</strong>
<br><strong>-Plantillas con almacenamiento persistente</strong>
<br><strong>-Despliegue de aplicaciones en cluster</strong>


</p>

<strong>Requisitos:</strong>
<br><strong>- Satellite</strong>
<br><strong>- RHEL client</strong>

<h3><br><strong>## Explorando templates</strong></h3>

<p align="left"><img src="https://github.com/gpulido-redhat/workshopclaro/raw/main/images/os023.png?raw=true?raw=true"></p>

Los templates permiten desplegar aplicaciones de forma rapida a traves del uso de parametros predefinidos, para ello primero nos logueamos al proyecto database0X y listamos los templates disponibles en la infraestrutura que coincidan con mariadb

<br>`# oc project database08`
<br>`# oc get template -n openshift | grep mariadb`

Para el primer ejemplo exploramos el template de mariadb que no es persistente por default
<br>`# oc describe template -n openshift mariadb-ephemeral`

Tambien podemos exportar el template a un archivo de texto que facilite su revision
<br>`# oc get template -n openshift mariadb-ephemeral -o yaml > template01.yaml`

Exploramos el template y sus variables, al menos revisemos las variables mas importan,tes, siendo estas
<br>MYSQL_USER
<br>MYSQL_PASSWORD
<br>MYSQL_DATABASE
<br>DATABASE_SERVICE_NAME

Podemos tambien revisar los parametros de la plantilla con el siguiente comando:
<br>`# oc process --parameters mariadb-ephemeral -n openshift`

Con las variables identificadas, podemos crear un despliegue con el comando
<br>`# oc new-app --template=mariadb-ephemeral --param=MYSQL_USER=admin --param=MYSQL_PASSWORD=redhat --param=MYSQL_DATABASE=mariadb --param=DATABASE_SERVICE_NAME=mariadb`

En caso de no tener errores publicamos la aplicacion con
<br>`# oc expose service/mariadb`

Una vez finalizado el despliegue, examinamos los pods
<br>`# oc get pods`

Hacemos una sesion al pod que no sea deploy o build
<br>`# oc rsh <pod>`

Hacemos algunas consultas mysql
<br>`mysql -u root`
<br>`show databases;`
<br>`exit`
<br>`exit`

Borramos el despliegue actual con los comandos
<br>`# oc delete all --all`
<br>`# oc delete secret mariadb`

Si volvemos a explorar el template, notamos que la imagen de mariadb esta basada en RHEL8, pero podemos cambiarla a una basada en RHEL7
<br>`# oc new-app --template=mariadb-ephemeral --param=MYSQL_USER=admin --param=MYSQL_PASSWORD=redhat --param=MYSQL_DATABASE=mariadb --param=DATABASE_SERVICE_NAME=mariadb --param=MARIADB_VERSION=10.3-el7`

Examinamos el despliegue y recuperamos los recursos con
<br>`# oc delete all --all`
<br>`# oc delete secret mariadb`

<h3><br><strong>## Laboratorio 01</strong></h3>
<br> - Exporte el template de mariadb-ephemeral en un archivo .yaml el cual examinara para identificar las variables
<br> - Despliegue el template de mariadb-ephemeral en su proyecto database0X
<br> - Recuepere los recursos con los comandos # oc delete all --all # oc delete secret mariadb
<br> - Despliegue el template de mariadb-ephemeral en su proyecto database0X tomando como imagen base el sistema RHEL7
<br> - Recuepere los recursos con los comandos # oc delete all --all # oc delete secret mariadb

<h3><br><strong>## Almacenamiento persistente</strong></h3>

<p align="left"><img src="https://github.com/gpulido-redhat/workshopclaro/raw/main/images/os024.png?raw=true?raw=true"></p>

Para desplegar aplicaciones con la opcion de almacenamiento persistente, primero debemos contar con un recurso de almacenamiento compartido, en este caso podemos examinar los recursos NFS remotos disponibles para el alumno0X, cada alumno tiene recursos NFS remotos desde el pvX1 hasta el pvX9, en este ejemplo crearemos el recurso pvX1

<br>`# sudo mkdir /tmp/pv81`
<br>`# sudo mount -o,ro 192.168.10.130:/nfs/pv81 /tmp/pv81`
<br>`# sudo df -h`

Ahora que validamos que si contamos con acceso NFS al recurso pvX1 podemos crear un archivo que permita configurar un PV (recuerde cambiar los parametros unicos)

Crear una carpeta llamada resources y dentro de ella colocar los archivos .yaml con los que se desplegara la base de datos.
<br>`# mkdir ~/resources/mysql`
<br>`# cd ~/resources/mysql`
<br>`# vi pv81.yaml`
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv81
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /nfs/pv81
    server: 192.168.10.130
```

Ahora creamos el PV con el comando
<br>`# oc create -f pv81.yaml`

Como la accion de crear un PV es una accion administrativa, no tendremos permiso para crearlo, por ello el instructor habilito el comando oc_create_pv para esta operacion
<br>`# oc_create_pv pv81.yaml`

El comando oc_create_pv desloguea al usuario actual, por lo tanto si desea seguir ejecutando comandos OC vuelva a loguearse a la plataforma con oc login -u alumno0X

Para verificar la creacion del PV ejecute
<br>`# oc get pv`

Al igual que la creacion del PV, listar los PV son comandos administrativos, por ello el instructor habilito el comando oc_list_pv
<br>`# oc_list_pv`

El comando oc_list_pv desloguea al usuario actual, por lo tanto si desea seguir ejecutando comandos OC vuelva a loguearse a la plataforma con oc login -u alumno0X

En caso fallara en la creacion de un PV, solicite al instructor borrar el PV creado de forma incorrecta o utilice sus otros recursos pvX

Con un PV creado, ya puede crear un PVC que utilice el recurso PV creado, esta accion no es administrativa, asi que puede ejecutarse en su proyecto directamente, para ello primero cree el archivo pvc.yaml, recuerde cambiar los parametros unicos

<br>`# vi pvc81.yaml`
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc81
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
  volumeName: pv81
```

Proceda con la creacion del pvc
<br>`# oc create -f pvc81.yaml`

Valide la creacion del pvc pasando del estado Pending al estado Bound
<br>`# oc get pvc`

Tambien puede validar que el PV y PVC esten asociados a su proyecto con el comando
<br>`# oc_list_pv`

El comando oc_list_pv desloguea al usuario actual, por lo tanto si desea seguir ejecutando comandos OC vuelva a loguearse a la plataforma con oc login -u alumno0X

Ahora con el PVC listo podemos crear un POD con la imagen de mysql:5.7 disponible desde el ftp del salon
<br>`# wget ftp://192.168.10.190/openshift/mysql.tar`
<br>`# podman load -i mysql.tar`
<br>`# podman images`

Cargamos la imagen de mysql:5.7 en nuestro espacio de trabajo
<br>`# oc import-image docker.io/library/mysql:5.7 --confirm`
<br>`# oc get is`

Ahora podemos crear el archivo de pod llamado mysql
<br>`# vi mysql.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  name: mysql
  labels:
    name: mysql
spec:
  containers:
    - resources:
        limits :
          cpu: 0.5
      image: mysql:5.7
      name: mysql
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: redhat
        - name: MYSQL_USER
          value: admin
        - name: MYSQL_PASSWORD
          value: redhat
        - name: MYSQL_DATABASE
          value: samples
      ports:
        - containerPort: 3306
          name: mysql
      volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
  volumes:
    - name: mysql-persistent-storage
      persistentVolumeClaim:
        claimName: pvc81
```
Notemos que el pod se define con almacenamiento persistente en /var/lib/mysql con el pvc81 que fue creado en pasos previos, creamos el pod con
<br>`# oc create -f mysql.yaml`

Con el pod creados podemos crear el servicio con
<br>`# vi service.yaml`
```
apiVersion: v1
kind: Service
metadata:
  labels:
    name: mysql
  name: mysql
spec:
  ports:
    - port: 3306
  selector:
    name: mysql
```
<br>`# oc create -f service.yaml`
<br>`# oc expose service mysql`

Con el servicio expuesto podemos examinar el pod con el usuario root y la clave redhat
<br>`# oc rsh mysql`
<br>`$ mysql -u root -p`
<br>`$ mysql> create database test01;`
<br>`$ mysql> create database test02;`
<br>`$ mysql> create database test03;`
<br>`$ exit;`
<br>`# exit;`

Validamos la data dentro de la carpeta /tmp/pv81
<br>`# ls /tmp/pv81;`

Ahora que verificamos la persistencia de la data, destruimos todos los recursos de openshift
<br>`# oc delete all --all;`

Intentamos crear de nuevo los recursos de imagen, pod, servicio y route
<br>`# oc import-image docker.io/library/mysql:5.7 --confirm`
<br>`# oc create -f mysql.yaml`
<br>`# oc create -f service.yaml`
<br>`# oc expose service mysql`

Verificamos la disponibilidad de las bases de datos creadas en pasos anteriores con el usuario root y la clave redhat<br>`# oc expose service mysql`
<br>`# oc rsh mysql`
<br>`$ mysql -u root -p`
<br>`$ show databases;`
<br>`$ exit`
<br>`# exit`

Con lo aprendido en el ejercicio anterior, creamos un PVC con los NFS 82 y 83

```
# vi pv82.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv82
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /nfs/pv82
    server: 192.168.10.130
# vi pv83.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv83
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /nfs/pv83
    server: 192.168.10.130

# vi pvc82.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc82
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
  volumeName: pv82

# vi pvc83.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc83
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
  volumeName: pv83

# oc_create_pv pv82.yaml
# oc_create_pv pv83yaml
# oc login -u alumno0X -p
# oc create -f pvc82.yaml
# oc create -f pvc83.yaml
# oc get pvc

NAME    STATUS   VOLUME   CAPACITY   ACCESS
pvc81   Bound    pv81     100Mi      RWX 
pvc82   Bound    pv82     100Mi      RWX 
pvc83   Bound    pv83     100Mi      RWX 
```

Con los pvc2 y pvc83 creamos un nuevo pod, pero que cuente con el disco default de mysql pero tambien un disco alterno para otros propositos
<br>`# vi mysql02.yaml`
```
apiVersion: v1
kind: Pod
metadata:
  name: mysql02
  labels:
    name: mysql02
spec:
  containers:
    - resources:
        limits :
          cpu: 0.5
      image: mysql:5.7
      name: mysql02
      env:
        - name: MYSQL_ROOT_PASSWORD
          value: redhat
        - name: MYSQL_USER
          value: admin
        - name: MYSQL_PASSWORD
          value: redhat
        - name: MYSQL_DATABASE
          value: samples
      ports:
        - containerPort: 3306
          name: mysql02
      volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
        - name: backup
          mountPath: /mnt
  volumes:
    - name: mysql-persistent-storage
      persistentVolumeClaim:
        claimName: pvc82
    - name: backup
      persistentVolumeClaim:
        claimName: pvc83
```

<br>`# vi service02.yaml`
```
apiVersion: v1
kind: Service
metadata:
  labels:
    name: mysql02
  name: mysql02
spec:
  ports:
    - port: 3306
  selector:
    name: mysql02
```

Creamos el pod, el servicio y route
<br>`# oc create -f mysql02.yaml`
<br>`# oc create -f service02.yaml`
<br>`# oc expose service mysql02`

Podemos describir el POD mysql02 y examinar las secciones de Volumes y Mounts con detalle en los Claims
<br>`# oc describe pod mysql02`

Ingresamos al pod mysql02 y creamos algunas bases de datos y luego un respaldo
<br>`# oc rsh mysql02`
<br>`# mysql -u root -p`
<br>`$ create database data01;`
<br>`$ create database data02;`
<br>`$ create database data03;`
<br>`$ exit;`
<br>`$ mysqldump -u root -p --all-databases > /mnt/databases.sql;`
<br>`# exit;`

Validamos la disponibilidad de datos montando los PV82 y PV82 en el sistema
<br>`# mkdir /tmp/pv82`
<br>`# mkdir /tmp/pv83`
<br>`# sudo mount -o,ro 192.168.10.130:/nfs/pv82 /tmp/pv82`
<br>`# sudo mount -o,ro 192.168.10.130:/nfs/pv83 /tmp/pv83`

La creacion de las bases de datos
<br>`# ls /tmp/pv82`

El respaldo de las bases de datos
<br>`# ls /tmp/pv83`

Eliminamos los recursos con
<br>`# oc delete all -all`

<h3><br><strong>## Laboratorio 02</strong></h3>
<br> - Utilizando los recursos en el ftp del salon, importe la imagen de mysql en version 5.7 en su espacio de trabajo
<br> - Usando los archivos .yaml de ejemplo, cree un servicio mysql persistente, recuerde usar su propia numeracion de recursos basado en pvX1-9 donde X es su numero de alumno
<br> - En caso se equivoque en crear algun recurso de PV y desee reahacerlo, use los demas recursos NFS disponibles o solicite a su instructor ayuda para borrar el recurso mal creado
<br> - Cree algunas bases de datos de ejemplo
<br> - Borre el pod, service y route de su proyecto e intente re-desplegarlo y verifique la persistentica de los datos
<br> - Crear un nuevo servicio de mysql que trabaje en paralelo con el primero y que ademas tenga un volumen persistente destinado a los respaldos
<br> - Al final del ejercicio borre los recursos con el coman oc delete all --all

<h3><br><strong>## Almacenamiento persistente con plantillas</strong></h3>

Muchas plantillas vienen listas para trabajar con almacenamiento persistente, por ejemplo exportamos y exploramos la plantilla de postgresql

<br>`# oc get template -n openshift postgresql-persistent -o yaml > template-postgresql-persistent`
<br>`# cat template-postgresql-persistent`

En el template observamos algunas variables criticas como los datos de conexion a la base de datos, la version de postgres y el volumen persistente default que coincide con service_name que vendria a ser postresql, por ello usamos algun NFS libre como el pvX4 para crear el pvc llamado postgresql

<br>`# mkdir ~/resources/postgresql`
<br>`# cd ~/resources/postgresql`
<br>`# vi pv84.yaml`
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv84
spec:
  capacity:
    storage: 100Mi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /nfs/pv84
    server: 192.168.10.130
```

<br>`# vi postgresql.yaml`
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgresql
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi
  volumeName: pv84
```

<br>`#  oc_create_pv pv84.yaml`
<br>`#  oc login -u alumno0X`
<br>`#  oc create -f postgresql.yaml`
<br>`#  oc get pvc`

Con el pvc postgresql ya podemos desplegar el template de postgresql-persistent
<br>`#  oc new-app --template=postgresql-persistent --param=POSTGRESQL_USER=admin --param=POSTGRESQL_PASSWORD=redhat --param=POSTGRESQL_DATABASE=samples`

Con el comando oc get pods buscamos el POD que no es buld o deploy y le hacemos sesion remota
<br>`#  oc get pod`
<br>`#  oc rsh <pod>`

Ejecutamos una consulta postgres para verificar la creacion de la base de datos postgres
<br>`$ psql`
<br>`$ create database pg01;`
<br>`$ create database pg02;`
<br>`$ create database pg03;`
<br>`$ \l`
<br>`$ \q`
<br>`$ exit`

Borramos todos los recursos con
<br>`oc delete all --all`
<br>`oc delete secrets postgresql`

Volvemos a enviar el comando de creacion de postgresql
<br>`#  oc new-app --template=postgresql-persistent --param=POSTGRESQL_USER=admin --param=POSTGRESQL_PASSWORD=redhat --param=POSTGRESQL_DATABASE=samples`

Verificamos la persistencia de datos con las bases de datos anteriores
<br>`#  oc get pod`
<br>`#  oc rsh <pod>`
<br>`$ psql`
<br>`$ \l`
<br>`$ \q`
<br>`$ exit`

Borramos todos los recursos con
<br>`oc delete all --all`
<br>`oc delete secrets postgresql`
<br>`oc delete pvc --all`

<h3><br><strong>## Laboratorio 03</strong></h3>
<br> - Explorar la plantilla postgresql-persistent e identificar los recursos necesarios para su uso
<br> - Desplegar la plantilla postgresql-persistent y crear bases de datos de ejemplo
<br> - Borrar los pods y demas recursos del proyecto, volver a desplegar los recursos y verificar la persistencia de datos

<h3><br><strong>## Desplegando Git persistente</strong></h3>

Para el uso permanente de un repositorio de codigo, preparamos un gogs personal, para ello primero creamos un proyecto nuevo
<br>`# oc new-project gogs0X`

El instructor ya cargo un template gogs en el espacio de trabajo openshift, el cual podemos exportar a un archivo o filtrar con los comandndos grep, donde destacamos la linea claimName
<br>`# oc get template -n openshift gogs -o yaml | grep claimName`
```
claimName: gogs-postgres-data
claimName: gogs-data
```

Creamos los PVC con esos nombres para ser usados por el template

```
# vi pv85.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv85
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /nfs/pv85
    server: 192.168.10.130
# vi pv86.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv86
spec:
  capacity:
    storage: 2Gi
  accessModes:
  - ReadWriteMany
  nfs:
    path: /nfs/pv86
    server: 192.168.10.130

# vi pvc-gogs-postgres-data.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gogs-postgres-data
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
  volumeName: pv85

# vi pvc-gogs-data.yaml

apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: gogs-data
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 2Gi
  volumeName: pv86

# oc_create_pv pv85.yaml
# oc_create_pv pv86yaml
# oc login -u alumno0X -p
# oc create -f pvc-gogs-postgres-data.yaml
# oc create -f pvc-gogs-data.yaml
# oc get pvc

```

Segun la documentacion del template, necesitamos 2 claim con al menos 1 Gb de espacio en cada uno, para ello usamos lo aprendido en ejercicios previos para obtener una salida similar a la siguiente
<br>`# oc get pvc`
```
NAME                 STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
gogs-data            Bound    pv86     2Gi        RWX                           12s
gogs-postgres-data   Bound    pv85     2Gi        RWX                           9s
```

En esta ruta colocar los archivos de pv y pvc de los demas ejemplos para crear estos recursos cambiando los datos.
<br>`# mkdir ~/resources/gogs`
<br>`# cd ~/resources/gogs`


Con los pvc disponibles con esos nombres exactos, podemos ejecutar el template con el parametro hostname obligatorio
<br>`# oc new-app --template=gogs --param=HOSTNAME=gogs0X.apps.cluster.opennovalabs.pe`

Esperamos la creacion de los recursos, sobre todo la salida del comando oc get pods, donde los pods que no incluyen deploy o build esten listos

abrimos un navegador con el hostname que ingresamos y podemos empesar a usar el gogs propio el cual estara disponible mientras los almacenamientos nfs esten activos
<p align="left"><img src="https://github.com/gpulido-redhat/workshopclaro/raw/main/images/os025.png?raw=true?raw=true"></p>


<h3><br><strong>## Laboratorio 04</strong></h3>
<br> - Cree los pvc necesarios para el template gogs usando los nfs que tenga disponibles, en caso de requerir nuevos nfs o depurar antiguos, solicitelo al instructor
<br> - Despliegue su propio gogs persistente en un proyecto nuevo con un url personal gogs0X.apps.cluster.opennovalabs.pe
<br> - Al finalizar el ejercicio depurar los recursos con
<br>`oc delete all --all`
<br>`oc delete pvc --all`

<h3><br><strong>## Publicando web seguras</strong></h3>

Repasemos el ejemplo de publicacion de webserver ingresando al proyecto website0X

<br>`# oc project website08`
<br>`# wget ftp://192.168.10.190/openshift/php-70-rhel7.tar`

Cargamos esta imagen en el podman local
<br>`# podman load -i php-70-rhel7.tar`

Verificamos la disponibilidad de la imagen
<br>`# podman image list`

Importamos esta imagen a nuestro proyecto website
<br>`# oc import-image registry.access.redhat.com/rhscl/php-70-rhel7 --confirm`
<br>`# oc get is`

Desplegamos esta imagen en un app llamado website y que su contruccion referencie a nuestro git con el contenido propio
<br>`# oc new-app php-70-rhel7~http://192.168.10.190:3000/alumno08/website.git --name=website`

Verificamos el despliegue de la aplicacion
<br>`# oc logs -f buildconfig/website`

Publicamos la aplicacion
<br>`# oc expose service/website`

Ingresamos por un navegador web y verificamos si la conexion cuenta con un certificado digital, en este caso no lo tiene
<p align="left"><img src="https://github.com/gpulido-redhat/workshopclaro/raw/main/images/os026.png?raw=true?raw=true"></p>

La ruta creada con el comando oc expose no es del tipo segura, por ello la borramos
<br>`# oc delete route website`

Descargamos del ftp del salon un archivo zipeado con un par de archivos de certificados
<br>`# wget ftp://192.168.10.190/openshift/certs.zip`
<br>`# unzip website.zip`

Cremoa una nueva ruta apuntando a estos archivos a un url personalizado
<br>`# oc create route edge --service website --key certs/tls.key --cert certs/tls.crt`
<br>`# oc get route`

Actualizamos la pagina y verificamos si ahora cuenta con una conexion cifrada por un certificado emitido para nuestro dominio apps.cluster.opennovalabs.pe
<p align="left"><img src="https://github.com/gpulido-redhat/workshopclaro/raw/main/images/os027.png?raw=true?raw=true"></p>
<p align="left"><img src="https://github.com/gpulido-redhat/workshopclaro/raw/main/images/os028.png?raw=true?raw=true"></p>

<h3><br><strong>## Laboratorio 05</strong></h3>
<br> - Vuelva a desplegar cualquier proyecto anterior que se base en http o php
<br> - Borre la ruta default del proyecto y cree una nueva con conexion cifrada por certificado digital
<br> - Al finalizar el ejercicio depurar los recursos con
<br>`oc delete all --all`

<h3><br><strong>## Ejercicio Demostrativo</strong></h3>
<br> - Cree un proyecto nuevo llamado wordpress0X
<br> - Descargue el archivo comprimido en ftp://192.168.10.190/openshift/wordpress_files.tar.gz en el encontrara archivos de configuracion en formato yaml con los cuales podra instalar un servicio de wordpress
<br> - Las imagenes de contenedores necesarias de mysql y wordpress se encuentran en ftp://192.168.10.190/openshift/
<br> - Utilice la siguiente secuencia logica para implementar el servicio wordpress, es importante que modifique los archivos *-pv.yaml ya que deben apuntar a sus recursos NFS personales, tambien el comando oc adm es un comando administrativo por ello el instructor habilito el script oc_adm_wp que simula el comando

```
Adapte el archivo mysql-pv.yaml para que apunte a sus recursos NFS
oc create -f mysql-pv.yaml
oc create -f mysql-pvc.yaml
oc create -f mysql-pod.yaml
oc create -f mysql-service.yaml
oc create serviceaccount wproot
El siguiente comando es administrativo, reemplazarlo con el script oc_adm_wp
oc adm policy add-scc-to-user anyuid -z wproot
Adapte el archivo wordpress-pv.yaml para que apunte a sus recursos NFS
oc create -f wordpress-pv.yaml
oc create -f wordpress-pvc.yaml
oc create -f wordpress-pod.yaml
oc create -f wordpress-service.yaml
oc expose svc wordpress --hostname=wordpress0X.apps.cluster.opennovalabs.pe
```

<br> - En caso de culminar este ejercicio ponga el marcha el wordpress desde un navegador web



<br>
<br>
<br>
<h3><a href="os">Volver</a></h3>
